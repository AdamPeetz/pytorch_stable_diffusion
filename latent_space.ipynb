{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers xformers accelerate\n",
    "!pip install -q numpy scipy ftfy Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a891ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "from IPython import display as IPdisplay\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import (\n",
    "    DDIMScheduler,\n",
    "    PNDMScheduler,\n",
    "    LMSDiscreteScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    EulerAncestralDiscreteScheduler,\n",
    "    EulerDiscreteScheduler,\n",
    ")\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b587b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e325cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, steps_offset=1)\n",
    "\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    scheduler=scheduler,\n",
    "    torch_dtype=torch.float32,\n",
    ").to(device)\n",
    "\n",
    "# Disable image generation progress bar, we'll display our own\n",
    "pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23fd211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficiency settings\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.unet.to(memory_format=torch.channels_last)\n",
    "pipe.vae.enable_slicing()\n",
    "pipe.vae.enable_tiling()\n",
    "pipe.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The seed is set to \"None\", because we want different results each time we run the generation.\n",
    "seed = 125\n",
    "\n",
    "if seed is not None:\n",
    "    generator = torch.manual_seed(seed)\n",
    "else:\n",
    "    generator = None\n",
    "\n",
    "# The guidance scale is set to its normal range (7 - 10).\n",
    "guidance_scale = 8\n",
    "\n",
    "# How long should the thing bake for\n",
    "num_inference_steps = 15\n",
    "\n",
    "\n",
    "# I would not recommend less than 512 on either dimension. This is because this model was trained on 512x512 image resolution.\n",
    "height = 960\n",
    "width = 960\n",
    "\n",
    "latents = torch.randn(\n",
    "    (1, pipe.unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b62215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text prompt that describes the desired output image.\n",
    "prompt_one = \"a steampunk city, industrial, pollution, drab, crisp, cityscape, factories, fractal, psytrance, high quality, masterpeice, highly detailed, elegant, sharp focus,concept art, character concepts, digital painting, scifi, fantasy, center frame, anatomically correct\"\n",
    "prompt_two = 'galaxies in space from a distance, fractal, psytrance, high quality, masterpeice, high definition, highly detailed, elegant, sharp focus, digital painting, scifi, fantasy, center frame'\n",
    "# A negative prompt that can be used to steer the generation away from certain features; here, it is empty.\n",
    "negative_prompt = \"extra limbs, bad art, floating, watermark, pencils, error, watermark, text, malformed, low detail, jpeg artifacts, cropped, plain background, ugly, low-res, poorly drawn face, out of frame, poorly drawn hands, blurry, bad art, extra hands, bad anatomy, amputee, missing limbs, amputated\"\n",
    "\n",
    "# Tokenizing and encoding the prompt into embeddings.\n",
    "prompt_one_tokens = pipe.tokenizer(\n",
    "    prompt_one,\n",
    "    padding=\"max_length\",\n",
    "    max_length=pipe.tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "prompt_one_embeds = pipe.text_encoder(prompt_one_tokens.input_ids.to(device))[0]\n",
    "\n",
    "# Tokenizing and encoding the prompt into embeddings.\n",
    "prompt_two_tokens = pipe.tokenizer(\n",
    "    prompt_two,\n",
    "    padding=\"max_length\",\n",
    "    max_length=pipe.tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "prompt_two_embeds = pipe.text_encoder(prompt_two_tokens.input_ids.to(device))[0]\n",
    "\n",
    "# Tokenizing and encoding the negative prompt into embeddings.\n",
    "if negative_prompt is None:\n",
    "    negative_prompt = [\"\"]\n",
    "\n",
    "negative_prompt_tokens = pipe.tokenizer(\n",
    "    negative_prompt,\n",
    "    padding=\"max_length\",\n",
    "    max_length=pipe.tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "negative_prompt_embeds = pipe.text_encoder(negative_prompt_tokens.input_ids.to(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slerp(t, v0, v1, DOT_THRESHOLD=0.9995):\n",
    "    v0 = v0.detach().cpu().numpy()\n",
    "    v1 = v1.detach().cpu().numpy()\n",
    "\n",
    "    #calculate dot product of vectors\n",
    "    dot = np.sum(v0 * v1 / (np.linalg.norm(v0) * np.linalg.norm(v1)))\n",
    "    #if transformation would not be meaningful, do linear interpolation\n",
    "    if np.abs(dot) > DOT_THRESHOLD:\n",
    "        v2 = (1 - t) * v0 + t * v1\n",
    "    # otherwise, spheical interpolate\n",
    "    else:\n",
    "        theta_0 = np.arccos(dot)\n",
    "        sin_theta_0 = np.sin(theta_0)\n",
    "        theta_t = theta_0 * t\n",
    "        sin_theta_t = np.sin(theta_t)\n",
    "        s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
    "        s1 = sin_theta_t / sin_theta_0\n",
    "        v2 = s0 * v0 + s1 * v1\n",
    "\n",
    "        v2 = torch.tensor(v2)\n",
    "        \n",
    "    return v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf307ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The higher you set this value, the smoother the interpolations will be. However, the generation time will increase. This value was chosen empirically.\n",
    "num_interpolation_steps = 150\n",
    "\n",
    "interpolation_list = []\n",
    "for _, t in enumerate(np.linspace(0, 1, num_interpolation_steps)):\n",
    "    interpolation_list.append(slerp(float(t), prompt_one_embeds, prompt_two_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff751b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "print(range(num_interpolation_steps))\n",
    "for batch in tqdm(range(num_interpolation_steps)):\n",
    "  images.append(pipe(\n",
    "                height=height,\n",
    "                width=width,\n",
    "                num_images_per_prompt=1,\n",
    "                prompt_embeds=interpolation_list[batch],\n",
    "                negative_prompt_embeds=negative_prompt_embeds,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=generator,\n",
    "                latents=latents,\n",
    "                     ).images\n",
    "      \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_save_images(base_directory, image_library, image_counter):\n",
    "  file_count = len(image_library)\n",
    "  globbed_dir_num = int((file_count/100)+1)\n",
    "  glob_file = 0\n",
    "  for flc in range(0, globbed_dir_num):\n",
    "      os.makedirs(base_directory + \"%05d\" % (flc+1))\n",
    "  for idx in tqdm(range(len(image_library))):\n",
    "    if idx  >= 0 and idx <= 100:\n",
    "         plt.figure(figsize=(10,10))\n",
    "         plt.imshow(image_library[idx][0])\n",
    "         plt.axis('off')\n",
    "         filename = base_directory + \"%05d/generated_plot_%05s.png\" % ((glob_file+1), (image_counter+1))\n",
    "         plt.savefig(filename, transparent=True,bbox_inches=\"tight\",pad_inches=0.0 )\n",
    "         plt.close()\n",
    "         image_counter += 1\n",
    "    if idx >= 101 and idx <= 200:\n",
    "         plt.figure(figsize=(10,10))\n",
    "         plt.imshow(image_library[idx][0])\n",
    "         plt.axis('off')\n",
    "         filename = base_directory + \"%05d/generated_plot_%05d.png\" % ((glob_file+2), (image_counter+1))\n",
    "         plt.savefig(filename, transparent=True,bbox_inches=\"tight\",pad_inches=0.0 )\n",
    "         plt.close()\n",
    "         image_counter += 1\n",
    "    if idx >= 201 and idx <= 300:\n",
    "         plt.figure(figsize=(10,10))\n",
    "         plt.imshow(image_library[idx][0])\n",
    "         plt.axis('off')\n",
    "         filename = base_directory + \"%05d/generated_plot_%05d.png\" % ((glob_file+3),( image_counter+1))\n",
    "         plt.savefig(filename, transparent=True,bbox_inches=\"tight\",pad_inches=0.0 )\n",
    "         plt.close()\n",
    "         image_counter += 1\n",
    "    if idx >= 301 and idx <= 400:\n",
    "         plt.figure(figsize=(10,10))\n",
    "         plt.imshow(image_library[idx][0])\n",
    "         plt.axis('off')\n",
    "         filename = base_directory + \"%05d/generated_plot_%05d.png\" % ((glob_file+4), (image_counter+1))\n",
    "         plt.savefig(filename, transparent=True,bbox_inches=\"tight\",pad_inches=0.0 )\n",
    "         plt.close()\n",
    "         image_counter += 1\n",
    "    if idx >= 401 and idx <= 500:\n",
    "         plt.figure(figsize=(10,10))\n",
    "         plt.imshow(image_library[idx][0])\n",
    "         plt.axis('off')\n",
    "         filename = base_directory + \"%05d/generated_plot_%05d.png\" % ((glob_file+5), (image_counter+1))\n",
    "         plt.savefig(filename, transparent=True,bbox_inches=\"tight\",pad_inches=0.0 )\n",
    "         plt.close()\n",
    "         image_counter += 1\n",
    "    if idx >= 501 and idx <= 600:\n",
    "         plt.figure(figsize=(10,10))\n",
    "         plt.imshow(image_library[idx][0])\n",
    "         plt.axis('off')\n",
    "         filename = base_directory + \"%05d/generated_plot_%05d.png\" % ((glob_file+6), (image_counter+1))\n",
    "         plt.savefig(filename, transparent=True,bbox_inches=\"tight\",pad_inches=0.0 )\n",
    "         plt.close()\n",
    "         image_counter += 1\n",
    "    if idx >= 601 and idx <= 700:\n",
    "         plt.figure(figsize=(10,10))\n",
    "         plt.imshow(image_library[idx][0])\n",
    "         plt.axis('off')\n",
    "         filename = base_directory + \"%05d/generated_plot_%05d.png\" % ((glob_file+7), (image_counter+1))\n",
    "         plt.savefig(filename, transparent=True,bbox_inches=\"tight\",pad_inches=0.0 )\n",
    "         plt.close()\n",
    "         image_counter += 1\n",
    "    if idx >= 701 and idx <= 800:\n",
    "         plt.figure(figsize=(10,10))\n",
    "         plt.imshow(image_library[idx][0])\n",
    "         plt.axis('off')\n",
    "         filename = base_directory + \"%05d/generated_plot_%05d.png\" % ((glob_file+8), (image_counter+1))\n",
    "         plt.savefig(filename, transparent=True,bbox_inches=\"tight\",pad_inches=0.0 )\n",
    "         plt.close()\n",
    "         image_counter += 1\n",
    "    if idx >= 801 and idx <= 900:\n",
    "         plt.figure(figsize=(10,10))\n",
    "         plt.imshow(image_library[idx][0])\n",
    "         plt.axis('off')\n",
    "         filename = base_directory + \"%05d/generated_plot_%05d.png\" % ((glob_file+9), (image_counter+1))\n",
    "         plt.savefig(filename, transparent=True,bbox_inches=\"tight\",pad_inches=0.0 )\n",
    "         plt.close()\n",
    "         image_counter += 1\n",
    "    if idx >= 901 and idx <= 1000:\n",
    "         plt.figure(figsize=(10,10))\n",
    "         plt.imshow(image_library[idx][0])\n",
    "         plt.axis('off')\n",
    "         filename = base_directory + \"%05d/generated_plot_%05d.png\" % ((glob_file+10), (image_counter+1))\n",
    "         plt.savefig(filename, transparent=True,bbox_inches=\"tight\",pad_inches=0.0 )\n",
    "         plt.close()\n",
    "         image_counter += 1\n",
    "    if idx >= 1001 and idx <= 1100:\n",
    "         plt.figure(figsize=(10,10))\n",
    "         plt.imshow(image_library[idx][0])\n",
    "         plt.axis('off')\n",
    "         filename = base_directory + \"%05d/generated_plot_%05d.png\" % ((glob_file+11), (image_counter+1))\n",
    "         plt.savefig(filename, transparent=True,bbox_inches=\"tight\",pad_inches=0.0 )\n",
    "         plt.close()\n",
    "         image_counter += 1\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f887cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "base_directory = '/content/gdrive/My Drive/stable_diffusion/'\n",
    "glob_save_images(base_directory, images, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9363f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ea160",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_base_folder = '/content/gdrive/My Drive/stable_diffusion/' # make sure to use your folder\n",
    "file_count = len(os.listdir(image_base_folder))\n",
    "print(file_count)\n",
    "\n",
    "video_name = 'test_960x960.avi'\n",
    "os.chdir(\"/content/gdrive/My Drive/stable_diffusion_outputs/\")\n",
    "\n",
    "image_locations = []\n",
    "for dir in range(1, file_count+1):\n",
    "  for image in os.listdir(image_base_folder + \"%05d/\" % (dir)):\n",
    "    if image.endswith(\".jpg\") or image.endswith(\".jpeg\") or image.endswith(\"png\"):\n",
    "      image_locations.append(image_base_folder + \"%05d/\" % (dir) + image)\n",
    "\n",
    "images = [img for img in image_locations\n",
    "              if img.endswith(\".jpg\") or\n",
    "                 img.endswith(\".jpeg\") or\n",
    "                 img.endswith(\"png\")]\n",
    "     \n",
    "    # Array images should only consider\n",
    "    # the image files ignoring others if any\n",
    " #   print(images) \n",
    "  \n",
    "frame = cv2.imread(images[0])\n",
    "  \n",
    "    # setting the frame width, height width\n",
    "    # the width, height of first image\n",
    "height, width, layers = frame.shape  \n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "video = cv2.VideoWriter(video_name, fourcc, 30,(width, height)) \n",
    "  \n",
    "    # Appending the images to the video one by one\n",
    "for image in images: \n",
    "    video.write(cv2.imread(image))\n",
    "      \n",
    "video.release()  # releasing the video generated\n",
    "\n",
    "# Deallocating memories taken for window creation\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb4da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "    # my all files starts with \"frame\" and ends with \".jpg\"\n",
    "fileList = glob.glob('/content/gdrive/My Drive/stable_diffusion/*/*')\n",
    "print(\"Number of files removed \",len(fileList))\n",
    "    \n",
    "for filePath in fileList:\n",
    "    os.remove(filePath)\n",
    "\n",
    "fileList = glob.glob('/content/gdrive/My Drive/stable_diffusion/*')    \n",
    "\n",
    "for filePath in fileList:\n",
    "    os.remove(filePath)\n",
    "\n",
    "print(\"Number of files removed \",len(fileList))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
